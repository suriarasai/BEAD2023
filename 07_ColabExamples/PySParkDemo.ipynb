{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP4BCkB9Cun+DSfvz8d7gb0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_6GrkimfmkM","executionInfo":{"status":"ok","timestamp":1697522802701,"user_tz":-480,"elapsed":55560,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}},"outputId":"7d118f06-09a2-4f63-a642-f4ad2d31a605"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# install pyspark using pip\n","!pip install --ignore-install -q pyspark\n","# install findspark using pip\n","!pip install --ignore-install -q findspark"]},{"cell_type":"code","source":["#from pyspark import SparkConf,SparkContext\n","from pyspark.sql import SparkSession\n","import collections\n","\n","spark = SparkSession.builder.master(\"local\").appName(\"Colab Demo for Oct 2023 EEP\").config('spark.ui.port', '4050').getOrCreate()"],"metadata":{"id":"-bDO7jzsgKyz","executionInfo":{"status":"ok","timestamp":1697522869882,"user_tz":-480,"elapsed":7589,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["data = [\"Welcome to the world of PySpark\",\n","        \"PySpark is a great tool\",\n","        \"Working with PySpark is fun\"]\n","\n","# Parallelizing the data with 2 partitions\n","rdd = spark.sparkContext.parallelize(data, 2)\n","print(rdd.count())\n"],"metadata":{"id":"xH-3N_qmhD40","executionInfo":{"status":"ok","timestamp":1697523075369,"user_tz":-480,"elapsed":3110,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"59167dad-d7b3-4706-8b7a-6875a84786c3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["3\n"]}]},{"cell_type":"code","source":["# FlatMap the rdd and split each line into words\n","words = rdd.flatMap(lambda line: line.split(\" \"))\n","\n","# Map each word to a pair of (word, 1)\n","wordCounts = words.map(lambda word: (word, 1))\n","\n","# Reduce the pairs by key (word) to count the occurrences of each word\n","counts = wordCounts.reduceByKey(lambda a, b: a + b)\n","\n","# Collect the result and print\n","for (word, count) in counts.collect():\n","    print(f\"{word}: {count}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gWpFf0zihizJ","executionInfo":{"status":"ok","timestamp":1697175193963,"user_tz":-480,"elapsed":4398,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}},"outputId":"992adfd1-01ab-4acb-f1e7-102aa666d42f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Welcome: 1\n","world: 1\n","of: 1\n","PySpark: 3\n","is: 2\n","to: 1\n","the: 1\n","a: 1\n","great: 1\n","tool: 1\n","Working: 1\n","with: 1\n","fun: 1\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uFzorV5qMwW4","executionInfo":{"status":"ok","timestamp":1697523339906,"user_tz":-480,"elapsed":3959,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}},"outputId":"83c8ef28-43d5-442c-e4cb-62a6b9985ae7"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Read the text file\n","rdd = spark.sparkContext.textFile('/content/drive/MyDrive/Sample.txt')\n","print(rdd.count())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jPqvJocUQ83w","executionInfo":{"status":"ok","timestamp":1697523341726,"user_tz":-480,"elapsed":499,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}},"outputId":"9a8fe3dd-1765-4069-8c53-5cfeb2b7cd74"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n"]}]},{"cell_type":"code","source":["# Read the text file\n","rdd = spark.sparkContext.textFile('/content/drive/MyDrive/Sample.txt')\n","# FlatMap the rdd and split each line into words\n","words = rdd.flatMap(lambda line: line.split(\" \"))\n","\n","# Map each word to a pair of (word, 1)\n","wordCounts = words.map(lambda word: (word, 1))\n","\n","# Reduce the pairs by key (word) to count the occurrences of each word\n","counts = wordCounts.reduceByKey(lambda a, b: a + b)\n","\n","# Collect the result and print\n","for (word, count) in counts.collect():\n","    print(f\"{word}: {count}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1liHKOoyl631","executionInfo":{"status":"ok","timestamp":1697178978025,"user_tz":-480,"elapsed":1660,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}},"outputId":"68612e0f-b722-4f20-992b-1b4d4fd86e7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Some: 1\n","random: 1\n","text: 1\n",": 2\n","1.1.2.: 1\n","The: 5\n","Excellence: 1\n","of: 13\n","Rain: 3\n","11.: 1\n","world: 3\n","its: 5\n","course: 1\n","maintains: 1\n","through: 1\n","life: 2\n","that: 5\n","rain: 10\n","unfailing: 1\n","gives;: 1\n","Thus: 2\n","is: 4\n","known: 1\n","the: 23\n","true: 1\n","ambrosial: 1\n","food: 3\n","all: 1\n","lives.: 1\n","By: 1\n","continuance: 1\n","preserved: 1\n","in: 6\n","existence;: 1\n","it: 4\n","therefore: 1\n","worthy: 1\n","to: 6\n","be: 7\n","called: 1\n","ambrosia.: 1\n","12.: 1\n","makes: 1\n","pleasant: 1\n","for: 1\n","eaters: 1\n","rise;: 1\n","As: 1\n","itself,: 1\n","thirst-quenching: 1\n","draught: 1\n","supplies.: 1\n","produces: 1\n","good: 1\n","food,: 1\n","and: 7\n","itself: 1\n","food.: 1\n","13.: 1\n","If: 12\n","clouds,: 2\n","promised: 1\n","rain,: 3\n","deceive,: 1\n","sky: 1\n","remain,: 1\n","Famine,: 1\n","sore: 1\n","torment,: 1\n","stalks: 1\n","o'er: 1\n","earth's: 1\n","vast: 1\n","ocean-girdled: 1\n","plain.: 1\n","cloud,: 1\n","withholding: 1\n","deceive: 1\n","(our: 1\n","hopes): 1\n","hunger: 1\n","will: 5\n","long: 1\n","distress: 1\n","sea-girt: 1\n","spacious: 2\n","world.: 2\n","14.: 1\n","clouds: 3\n","their: 2\n","wealth: 3\n","waters: 1\n","fail: 2\n","on: 2\n","earth: 2\n","pour,: 1\n","ploughers: 1\n","plough: 2\n","with: 2\n","oxen's: 1\n","sturdy: 1\n","team: 1\n","no: 5\n","more.: 1\n","abundance: 1\n","imparting: 1\n","diminish,: 1\n","labour: 1\n","must: 1\n","cease.: 1\n","15.: 1\n","'Tis: 2\n","works: 1\n","all:: 1\n","ruin: 1\n","spreads,: 1\n","then: 1\n","timely: 1\n","aid: 1\n","supplies;: 1\n","As,: 1\n","happy: 1\n","days: 1\n","before,: 1\n","bids: 1\n","ruined: 1\n","rise.: 1\n","by: 3\n","absence: 1\n","ruins: 1\n","men;: 1\n","existence: 1\n","restores: 1\n","them: 2\n","fortune.: 1\n","16.: 1\n","from: 2\n","drops: 1\n","are: 1\n","shed.: 1\n","rare: 1\n","see: 1\n","green: 2\n","herb: 1\n","lift: 1\n","up: 2\n","head.4: 1\n","drop: 1\n","falls: 1\n","not: 3\n","even: 1\n","blade: 1\n","grass: 1\n","seen.: 1\n","17.: 1\n","restrain: 1\n","gifts: 1\n","grant: 1\n","treasures: 2\n","ocean's: 1\n","wide: 3\n","domain.: 1\n","Even: 1\n","sea: 1\n","diminished,: 1\n","if: 1\n","cloud: 1\n","has: 1\n","drawn: 1\n","(its: 1\n","waters): 1\n","gives: 1\n","back: 1\n","again: 1\n","(in: 1\n","rain).: 1\n","18.: 1\n","heaven: 3\n","grow: 1\n","dry,: 1\n","feast: 1\n","offering: 1\n","never: 1\n","more,: 1\n","Will: 1\n","men: 2\n","heavenly: 1\n","ones: 1\n","adore.: 1\n","dry: 1\n","up,: 1\n","neither: 1\n","yearly: 1\n","festivals,: 1\n","nor: 1\n","daily: 1\n","worship: 1\n","offered: 1\n","this: 2\n","world,: 1\n","celestials.: 1\n","19.: 1\n","watery: 1\n","ceases: 1\n","dispense,: 1\n","Through: 1\n","cease: 1\n","gifts,: 1\n","deeds: 1\n","'penitence'.: 1\n","fall: 1\n","not,: 1\n","penance: 1\n","alms-deeds: 1\n","dwell: 1\n","within: 1\n","20: 1\n","When: 1\n","water: 1\n","fails,: 2\n","functions: 1\n","nature: 1\n","cease,: 1\n","you: 1\n","say;: 1\n","when: 1\n","can: 1\n","walk: 1\n","'duty's: 1\n","ordered: 1\n","way'.: 1\n","said: 1\n","duties: 1\n","cannot: 2\n","discharged: 1\n","any: 1\n","person: 1\n","without: 2\n","water,: 1\n","so: 1\n","there: 1\n","flowing: 1\n","water.: 1\n"]}]},{"cell_type":"code","source":["# Load the data\n","url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n","from pyspark import SparkFiles\n","# Add the file to Spark jobs\n","spark.sparkContext.addFile(url)\n","# Read the CSV file into a DataFrame\n","df = spark.read.csv(\"file://\"+SparkFiles.get(\"iris.data\"), inferSchema=True, header=False)\n","# Show the first few rows of the DataFrame\n","df.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KUq4tJVwyDlt","executionInfo":{"status":"ok","timestamp":1697179534095,"user_tz":-480,"elapsed":14808,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}},"outputId":"2768089d-38ae-40af-cc99-dcc49ddfd117"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---+---+---+-----------+\n","|_c0|_c1|_c2|_c3|        _c4|\n","+---+---+---+---+-----------+\n","|5.1|3.5|1.4|0.2|Iris-setosa|\n","|4.9|3.0|1.4|0.2|Iris-setosa|\n","|4.7|3.2|1.3|0.2|Iris-setosa|\n","|4.6|3.1|1.5|0.2|Iris-setosa|\n","|5.0|3.6|1.4|0.2|Iris-setosa|\n","|5.4|3.9|1.7|0.4|Iris-setosa|\n","|4.6|3.4|1.4|0.3|Iris-setosa|\n","|5.0|3.4|1.5|0.2|Iris-setosa|\n","|4.4|2.9|1.4|0.2|Iris-setosa|\n","|4.9|3.1|1.5|0.1|Iris-setosa|\n","|5.4|3.7|1.5|0.2|Iris-setosa|\n","|4.8|3.4|1.6|0.2|Iris-setosa|\n","|4.8|3.0|1.4|0.1|Iris-setosa|\n","|4.3|3.0|1.1|0.1|Iris-setosa|\n","|5.8|4.0|1.2|0.2|Iris-setosa|\n","|5.7|4.4|1.5|0.4|Iris-setosa|\n","|5.4|3.9|1.3|0.4|Iris-setosa|\n","|5.1|3.5|1.4|0.3|Iris-setosa|\n","|5.7|3.8|1.7|0.3|Iris-setosa|\n","|5.1|3.8|1.5|0.3|Iris-setosa|\n","+---+---+---+---+-----------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["# Count the number of rows\n","print(f\"Total rows: {df.count()}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rj9I0Ev0yiQl","executionInfo":{"status":"ok","timestamp":1697179653715,"user_tz":-480,"elapsed":1201,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}},"outputId":"512a8e65-25ac-455f-bbb3-35b9291d59c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total rows: 150\n"]}]},{"cell_type":"code","source":["from typing import Tuple, Union, Any, List\n","\n","def extend_tuple(input_tuple: Tuple[Any, Any, Any]) -> Tuple[Any, str, Any, str, Any, str]:\n","    \"\"\"\n","    This function takes a 3-sized tuple and returns a 6-sized tuple,\n","    with each original parameter followed by its String representation.\n","\n","    Args:\n","    - input_tuple: A 3-sized tuple with any data type.\n","\n","    Returns:\n","    A 6-sized tuple.\n","    \"\"\"\n","    output_tuple = ()\n","    for item in input_tuple:\n","        output_tuple += (item, str(item))\n","    return output_tuple\n","\n","# Example usage with explicit types:\n","input_value: Tuple[bool, float, str] = (True, 22.25, \"yes\")\n","output_value: Tuple[bool, str, float, str, str, str] = extend_tuple(input_value)\n","\n","# Displaying the result\n","print(output_value)\n"],"metadata":{"id":"eiJtMApl6OiD","executionInfo":{"status":"ok","timestamp":1697181661004,"user_tz":-480,"elapsed":468,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}},"outputId":"324485f0-7b1e-4edc-8923-1fedb619a89c","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(True, 'True', 22.25, '22.25', 'yes', 'yes')\n"]}]}]}