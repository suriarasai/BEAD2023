{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNVu6JlbJuDgXuIN7FF3rIv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Objective\n","This lab will demonstrate to participants how to create Apache Spark core constructs. The tutorial covers some of the essential transformations and actions in PySpark Core. Since it runs on the Google cloud, we don’t need to install anything in our system locally.\n"],"metadata":{"id":"D9f32Bi7ENM9"}},{"cell_type":"markdown","source":["##Tutorials\n","We need to install all the dependencies in Google Colab environment such as Apache Spark, Hadoop, and PySpark."],"metadata":{"id":"CBIScGCeEZHl"}},{"cell_type":"code","source":["# install pyspark using pip\n","!pip install --ignore-install -q pyspark\n","# install findspark using pip\n","!pip install --ignore-install -q findspark\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PfE41-jaEicn","executionInfo":{"status":"ok","timestamp":1697518932655,"user_tz":-480,"elapsed":54918,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}},"outputId":"f9f6f6b3-bd82-4a25-8794-c86d9f5048a6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","import collections\n","spark = SparkSession.builder.master(\"local\").appName(\"Colab Demo for Actions and Transofrmations\").config('spark.ui.port', '4050').getOrCreate()\n"],"metadata":{"id":"1rhl7EY9FiRR","executionInfo":{"status":"ok","timestamp":1697518962047,"user_tz":-480,"elapsed":9272,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Transformations\n","\n","Transformations are operations on RDDs or DataSet or DataFrame that produce another similar structure.\n","\n","### `map`\n","\n","Applies a function to each element of the RDD. PySpark's map function is used to apply a function to each element of an RDD (Resilient Distributed Dataset). Here are some examples to illustrate how to use the map function in PySpark:\n","\n","Basic Usage:"],"metadata":{"id":"qi0pBvVQFGJ2"}},{"cell_type":"code","source":["data = [1, 2, 3, 4, 5]\n","rdd = spark.sparkContext.parallelize(data)\n","\n","# Multiply each element by 2\n","multiplied = rdd.map(lambda x: x * 2)\n","print(multiplied.collect())  # [2, 4, 6, 8, 10]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZGEgjX72FbL0","executionInfo":{"status":"ok","timestamp":1697503454126,"user_tz":-480,"elapsed":2906,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}},"outputId":"4cb58b70-4ea0-4073-d838-3abbd4c26aa0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2, 4, 6, 8, 10]\n"]}]},{"cell_type":"markdown","source":["Working with Tuples:"],"metadata":{"id":"4i6MY2hkF_Jd"}},{"cell_type":"code","source":["data_tuples = [(1, 'a'), (2, 'b'), (3, 'c')]\n","rdd_tuples = spark.sparkContext.parallelize(data_tuples)\n","\n","# Add 1 to the number in each tuple\n","added = rdd_tuples.map(lambda x: (x[0] + 1, x[1]))\n","print(added.collect())  # [(2, 'a'), (3, 'b'), (4, 'c')]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WRTizKcvGBO6","executionInfo":{"status":"ok","timestamp":1697503530638,"user_tz":-480,"elapsed":963,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}},"outputId":"319c85ec-345d-4d58-8d93-b595904f3284"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(2, 'a'), (3, 'b'), (4, 'c')]\n"]}]},{"cell_type":"markdown","source":["String Manipulation:"],"metadata":{"id":"mBzNYqgZGNps"}},{"cell_type":"code","source":["data_strings = [\"Hello\", \"PySpark\", \"World\"]\n","rdd_strings = spark.sparkContext.parallelize(data_strings)\n","\n","# Convert each string to uppercase\n","uppercase = rdd_strings.map(lambda x: x.upper())\n","print(uppercase.collect())  # ['HELLO', 'PYSPARK', 'WORLD']\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gu67fV6uGOPF","executionInfo":{"status":"ok","timestamp":1697503586620,"user_tz":-480,"elapsed":821,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}},"outputId":"8420aaf4-c4bf-4bc0-a379-e53f03f29c5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['HELLO', 'PYSPARK', 'WORLD']\n"]}]},{"cell_type":"markdown","source":["Using Multiple Operations:"],"metadata":{"id":"B4CayMpVGe0Z"}},{"cell_type":"code","source":["numbers = [1, 2, 3, 4, 5]\n","rdd_numbers = spark.sparkContext.parallelize(numbers)\n","\n","# Perform a series of operations: square, then add 2\n","result = rdd_numbers.map(lambda x: x**2).map(lambda y: y + 2)\n","print(result.collect())  # [3, 6, 11, 18, 27]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n3YDssQ7FHKW","executionInfo":{"status":"ok","timestamp":1697503669113,"user_tz":-480,"elapsed":861,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}},"outputId":"5fd18161-38d3-4895-e91f-381b28f6c2dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[3, 6, 11, 18, 27]\n"]}]},{"cell_type":"markdown","source":["Using Custom Function"],"metadata":{"id":"tTQZw1SjGyPd"}},{"cell_type":"code","source":["def square_and_add_one(n):\n","    return n**2 + 1\n","\n","numbers = [1, 2, 3, 4, 5]\n","rdd_numbers = spark.sparkContext.parallelize(numbers)\n","\n","result = rdd_numbers.map(square_and_add_one)\n","print(result.collect())  # [2, 5, 10, 17, 26]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KUFyWdCUG0cB","executionInfo":{"status":"ok","timestamp":1697503738402,"user_tz":-480,"elapsed":819,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}},"outputId":"4a2abc9d-9768-48a2-9354-4c6a93d74702"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2, 5, 10, 17, 26]\n"]}]},{"cell_type":"markdown","source":["Lambda Inline Functions"],"metadata":{"id":"-8YBmoxCHCis"}},{"cell_type":"code","source":["x = [1, 2, 3, 4, 5]\n","squared_rdd = rdd.map(lambda x: x*x)\n","print(squared_rdd.collect()) # [1, 4, 9, 16, 25]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jj9QH93QHFD6","executionInfo":{"status":"ok","timestamp":1697503845894,"user_tz":-480,"elapsed":613,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}},"outputId":"afecefa3-ad4d-4b2c-d786-81991ec4b42c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 4, 9, 16, 25]\n"]}]},{"cell_type":"markdown","source":["### flatMap\n","The `flatMap` transformation in PySpark returns multiple values for each element in the source RDD. The output is flattened so that you get a single RDD rather than an RDD of lists or arrays.  Here are some examples using `flatMap` in PySpark:\n","Basic Text Processing:\n","Let's say you have an RDD of sentences and you want to split them into words:\n"],"metadata":{"id":"ebGDxppzAaya"}},{"cell_type":"code","source":["sentences = [\"Hello world\", \"I am learning PySpark\"]\n","rdd_sentences = spark.sparkContext.parallelize(sentences)\n","words = rdd_sentences.flatMap(lambda x: x.split(\" \"))\n","print(words.collect())  # ['Hello', 'world', 'I', 'am', 'learning', 'PySpark']\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"69z0ehPHAhK4","executionInfo":{"status":"ok","timestamp":1697518976157,"user_tz":-480,"elapsed":2305,"user":{"displayName":"Suria R Asai","userId":"11491150432218950679"}},"outputId":"cb884ad3-ad74-48af-eb19-2ffa275a57e7"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', 'world', 'I', 'am', 'learning', 'PySpark']\n"]}]}]}